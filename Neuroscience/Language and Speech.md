[Parallel processing in speech perception with local and global representations of linguistic context](https://doi.org/10.7554/eLife.72056) #bro22 (2022)

"Thus, there is little doubt that the brain uses context to facilitate processing of upcoming input, at multiple levels of representation. Here we investigate a fundamental question about the underlying cognitive organization: Does the brain develop a single, unified representation of the input? In other words, one representation that is consistent across hierarchical levels, effectively propagating information from the sentence context across hierarchical levels to anticipate even low-level features of the sensory input such as phonemes? Or do cognitive subsystems differ in the extent and kind of context they use to interpret their input?" p. 3

![[bro22 fig 1 - local and unified architectures.png]]

**Figure 1** Information flow in local and unified architectures for speech processing


## Discussion

"The present MEG data provide clear evidence for the existence of a neural representation of speech that is unified across representational hierarchies. This representation incrementally integrates phonetic input with information from the multi-word context within about 100 ms. However, in addition to this globally unified representation, brain responses also show evidence of separate neural representations that use more local contexts to process the same input." p. 19

"A second key result from this study, however, is evidence that this unified model is not the only representation of speech. Brain responses also exhibited evidence for two other, separate functional systems that process incoming phonemes while building representations that incorporate different, more constrained kinds of context: one based on a local word context, processing the current word with a prior based on context-independent lexical frequencies, and another based on the local phoneme sequence regardless of word boundaries. Each of these three functional systems generates its own predictions for upcoming phonemes, resulting in parallel responses to phoneme entropy. Each system is updated incrementally at the phoneme rate, reflected in early responses to surprisal. However, each system engages an at least partially different configuration of neural sources, as evidenced by the localization results."

"Together, these results suggest that multiple predictive models process speech input in parallel. An architecture consistent with these observations is sketched in Figure 8: three different neural systems receive the speech input in parallel. Each representation is updated incrementally by arriving phonemes. However, the three systems differ in the extent and kind of context that they incorporate, each generating its own probabilistic beliefs about the current word and/or future phonemes. For instance, the sublexical model uses the local phoneme history to predict upcoming phonemes. The updates are incremental because the state of the model at time $k+1$ is determined by the state of the model at time $k$ and the phoneme input from time $k$. The same incremental update pattern applies to the sublexical, word and sentence models."

![[bro22 fig 8 - parallel context models.png]]

**Figure 8** An architecture for speech perception with multiple parallel context models


### Implications for word recognition

"Perhaps the most surprising implication of our results is that multiple probabilistic cohort representations seem to be maintained in parallel." p. 21

"A second implication is that feedback from the sentence level context can and does affect phoneme processing. The observed phoneme entropy effects indicate that phoneme level expectations are modulated by the sentence context." p. 22


### Conclusions

"Prior research on the use of context during language processing has often focused on binary distinctions, such as asking whether context is or is not used to predict future input. Such questions assumed a single serial or cascaded processing stream. Here we show that this assumption might have been misleading, because different predictive models are maintained in parallel. Our results suggest that robust speech processing is based on probabilistic predictions using different context models in parallel, and cutting across hierarchical levels of representations." p. 23

---

[Visual and linguistic semantic representations are aligned at the border of human visual cortex](https://doi.org/10.1038/s41593-021-00921-6) (2021)
* Visual and linguistic representations are *aligned in the brain*
* "Remarkably, the pattern of semantic selectivity in these two distinct networks corresponded along the boundary of visual cortex: for visual categories represented posterior to the boundary, **the same categories were represented linguistically on the anterior side**. These results suggest that these two networks are smoothly joined to form one contiguous map." (Abstract)
